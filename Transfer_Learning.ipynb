{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "077a7019-fbe1-444b-b600-fccf35257a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9242a52f-9e1a-40bc-9b8a-35eaa6c5dfb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84805706-ea01-4fd0-8acd-467301d09c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/cola (download: 368.14 KiB, generated: 596.73 KiB, post-processed: Unknown size, total: 964.86 KiB) to C:\\Users\\49397\\.cache\\huggingface\\datasets\\glue\\cola\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749297a94bf54ea5a162c6d1050fa6ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=376971.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to C:\\Users\\49397\\.cache\\huggingface\\datasets\\glue\\cola\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 8551\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1043\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1063\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The Corpus of Linguistic Acceptability consists of English acceptability judgments \n",
    "drawn from books and journal articles on linguistic theory. \n",
    "Each example is a sequence of words \n",
    "annotated with whether it is a grammatical English sentence.\n",
    "\"\"\"\n",
    "raw_datasets = load_dataset('glue', 'cola')\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcb6221d-6b45-4fcd-8aed-279d49012371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': \"Our friends won't buy this analysis, let alone the next one we propose.\",\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "545493fe-511b-4ba7-886e-fc064251061e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'input_ids': array([[ 101, 2256, 2814, ...,    0,    0,    0],\n",
       "         [ 101, 2028, 2062, ...,    0,    0,    0],\n",
       "         [ 101, 2028, 2062, ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101, 2009, 2003, ...,    0,    0,    0],\n",
       "         [ 101, 1045, 2018, ...,    0,    0,    0],\n",
       "         [ 101, 2054, 2035, ...,    0,    0,    0]]),\n",
       "  'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]]),\n",
       "  'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0]])},\n",
       " 'validation': {'input_ids': array([[  101,  1996, 11279, ...,     0,     0,     0],\n",
       "         [  101,  1996, 15871, ...,     0,     0,     0],\n",
       "         [  101,  1996,  6228, ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2198,  4149, ...,     0,     0,     0],\n",
       "         [  101,  2198,  5412, ...,     0,     0,     0],\n",
       "         [  101,  2198,  5720, ...,     0,     0,     0]]),\n",
       "  'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]]),\n",
       "  'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0]])},\n",
       " 'test': {'input_ids': array([[  101,  3021, 26265, ...,     0,     0,     0],\n",
       "         [  101,  1996,  2482, ...,     0,     0,     0],\n",
       "         [  101,  3021,  3724, ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2198,  2354, ...,     0,     0,     0],\n",
       "         [  101,  2198,  2354, ...,     0,     0,     0],\n",
       "         [  101,  1037,  3861, ...,     0,     0,     0]]),\n",
       "  'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]]),\n",
       "  'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0]])}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    encoded = tokenizer(\n",
    "        dataset['sentence'],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='np',\n",
    "    )\n",
    "    return encoded.data\n",
    "\n",
    "tokenized_datasets = {split: tokenize_dataset(raw_datasets[split]) for split in raw_datasets.keys()}\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71031a97-9b36-4a4c-b148-e8dcc3d59e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8 # if batch size is too big, it will cause OOM on GPU\n",
    "num_epochs = 5\n",
    "num_train_steps = (len(tokenized_datasets['train']['input_ids']) // batch_size) * num_epochs\n",
    "lr_scheduler = PolynomialDecay(\n",
    "    initial_learning_rate=5e-5, # transformer models benefit from a much lower learning rate than the default for Adam\n",
    "    end_learning_rate=0.,\n",
    "    decay_steps=num_train_steps\n",
    "    )\n",
    "optimizer = Adam(learning_rate=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f518c225-6246-4099-9237-c1daeafa33c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f412661a-5b49-4ecf-ba30-060222742947",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf4a1e0c-afc3-4675-b600-ae9c2fb214d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1069/1069 [==============================] - 229s 195ms/step - loss: 0.5127 - accuracy: 0.7494 - val_loss: 0.4661 - val_accuracy: 0.7881\n",
      "Epoch 2/5\n",
      "1069/1069 [==============================] - 208s 194ms/step - loss: 0.3138 - accuracy: 0.8712 - val_loss: 0.4728 - val_accuracy: 0.8025\n",
      "Epoch 3/5\n",
      "1069/1069 [==============================] - 210s 197ms/step - loss: 0.1443 - accuracy: 0.9482 - val_loss: 0.6262 - val_accuracy: 0.8082\n",
      "Epoch 4/5\n",
      "1069/1069 [==============================] - 210s 196ms/step - loss: 0.0710 - accuracy: 0.9753 - val_loss: 0.7407 - val_accuracy: 0.8169\n",
      "Epoch 5/5\n",
      "1069/1069 [==============================] - 208s 194ms/step - loss: 0.0315 - accuracy: 0.9887 - val_loss: 0.9293 - val_accuracy: 0.8169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d35e2edfd0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    tokenized_datasets['train'],\n",
    "    np.array(raw_datasets['train']['label']),\n",
    "    validation_data=(tokenized_datasets['validation'], np.array(raw_datasets['validation']['label'])),\n",
    "    batch_size=batch_size,\n",
    "    epochs=num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6022f46d-0e62-47e4-ae66-c2ec1488946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(tokenized_datasets['validation'])['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d33e37b5-2a37-460a-b37c-3056a90c9318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "nongrammatical       0.78      0.57      0.66       322\n",
      "   grammatical       0.83      0.93      0.87       721\n",
      "\n",
      "      accuracy                           0.82      1043\n",
      "     macro avg       0.80      0.75      0.77      1043\n",
      "  weighted avg       0.81      0.82      0.81      1043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(preds, axis=1)\n",
    "y_true = raw_datasets['validation']['label']\n",
    "print(classification_report(y_true, y_pred, target_names=['nongrammatical', 'grammatical']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
